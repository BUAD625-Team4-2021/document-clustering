{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Identifying\n",
    "### Running through the tweets, based on Harry Wang's Document Clustering \n",
    "<p> We take the tweets out of the data, clean them of html, hashtags and mentions, then run the text through a tokenizer before identifying the most common words/phrases throughout the tweets. Those common words/phrases are then the aspects we base the clustering of the tweets on. As of right now, the model is using singular terms with a fixed dictionary, the hope is to move towards a dynamic dictionary that grows with the new data recieved.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p #https://pypi.org/project/tweet-preprocessor/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Tweets.csv')\n",
    "tweets = df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica What @dhepburn said.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = p.clean(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What said.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets = []\n",
    "for tweet in tweets:\n",
    "    tweet = BeautifulSoup(tweet, 'html.parser').getText()\n",
    "    tweet = p.clean(tweet)\n",
    "    new_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = new_tweets\n",
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in tweets:\n",
    "    allwords_stemmed = tokenize_and_stem(i) # for each item in 'tweets', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) # extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Code\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.78 s\n",
      "(14640, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.1, min_df=.001,\n",
    "                                   stop_words='english', max_features=1000,\n",
    "                                   strip_accents='unicode', use_idf=True, \n",
    "                                   tokenizer=tokenize_and_stem, ngram_range=(1,2))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(tweets) #fit the vectorizer to tweets\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "#print(terms)\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.09 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10759\n",
       "3     1211\n",
       "0     1158\n",
       "4      892\n",
       "2      620\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets = { 'tweet': tweets, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(Tweets, index = [clusters])\n",
    "\n",
    "#print(frame)\n",
    "\n",
    "frame['cluster'].value_counts() #number of tweets per cluster (clusters from 0 to 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterwords = pd.DataFrame(columns = ['Cluster', 'Word', \"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0 words: help,\n",
      "\n",
      "Cluster 1 words: hour,\n",
      "\n",
      "Cluster 2 words: service,\n",
      "\n",
      "Cluster 3 words: 's,\n",
      "\n",
      "Cluster 4 words: cancelled,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    name = \"Cluster %d words:\" % i\n",
    "    \n",
    "    for ind in order_centroids[i, :1]:\n",
    "        word = ' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0]\n",
    "        print(word, end=',')\n",
    "        clusterwords.loc[len(clusterwords.index)] = [i, word, \"Miscellaneous\"]\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = {'Customer Service':['service', 'customer', 'phone'], \n",
    "                       'Delays':['delayed', 'hour', 'hold', 'waited', 'late'], \n",
    "                       'Cancelled Flight(s)':['cancelled', 'rebook'], \n",
    "                       'Please Help':['help', 'please', 'need'], \n",
    "                       'Physical location':['bag', 'gate', 'check', 'agent', 'lost', 'connecting']\n",
    "                      }\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in cluster_assignments.items():\n",
    "        for item in value:\n",
    "            if (val == item):\n",
    "                return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    cluster = clusterwords[clusterwords.isin([i]).any(axis=1)]\n",
    "    tof = False\n",
    "    word = cluster['Word'].values[0]\n",
    "    word = word.replace(\" \", \"\")\n",
    "    cluster_name = \"Miscellaneous\"\n",
    "    for value in cluster_assignments.values():\n",
    "        for item in value:\n",
    "            if (word == item):\n",
    "                tof = True\n",
    "                cluster_name = get_key(word)\n",
    "                break\n",
    "    clusterwords.loc[(clusterwords['Cluster'] == i), 'Name'] = cluster_name\n",
    "    frame.loc[(frame['cluster'] == i), 'reason'] = cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Word</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>help</td>\n",
       "      <td>Please Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hour</td>\n",
       "      <td>Delays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>service</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>'s</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>Cancelled Flight(s)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cluster        Word                 Name\n",
       "0       0        help          Please Help\n",
       "1       1        hour               Delays\n",
       "2       2     service     Customer Service\n",
       "3       3          's        Miscellaneous\n",
       "4       4   cancelled  Cancelled Flight(s)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cluster</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What said.</td>\n",
       "      <td>1</td>\n",
       "      <td>Delays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus you've added commercials to the experienc...</td>\n",
       "      <td>1</td>\n",
       "      <td>Delays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I didn't today... Must mean I need to take ano...</td>\n",
       "      <td>1</td>\n",
       "      <td>Delays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's really aggressive to blast obnoxious \"ent...</td>\n",
       "      <td>3</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>3</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thank you we got on a different flight to Chic...</td>\n",
       "      <td>1</td>\n",
       "      <td>Delays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leaving over minutes Late Flight. No warnings ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please bring American Airlines to</td>\n",
       "      <td>0</td>\n",
       "      <td>Please Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you have my money, you change my flight, and d...</td>\n",
       "      <td>1</td>\n",
       "      <td>Delays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we have ppl so we need know how many seats are...</td>\n",
       "      <td>1</td>\n",
       "      <td>Delays</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  cluster         reason\n",
       "1                                          What said.        1         Delays\n",
       "1   plus you've added commercials to the experienc...        1         Delays\n",
       "1   I didn't today... Must mean I need to take ano...        1         Delays\n",
       "3   it's really aggressive to blast obnoxious \"ent...        3  Miscellaneous\n",
       "3            and it's a really big bad thing about it        3  Miscellaneous\n",
       "..                                                ...      ...            ...\n",
       "1   thank you we got on a different flight to Chic...        1         Delays\n",
       "3   leaving over minutes Late Flight. No warnings ...        3  Miscellaneous\n",
       "0                   Please bring American Airlines to        0    Please Help\n",
       "1   you have my money, you change my flight, and d...        1         Delays\n",
       "1   we have ppl so we need know how many seats are...        1         Delays\n",
       "\n",
       "[14640 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://codereview.stackexchange.com/questions/249329/finding-the-most-frequent-words-in-pandas-dataframe\n",
    "# for i in range(num_clusters):\n",
    "#     words = clusterwords[clusterwords.isin([i]).any(axis=1)]\n",
    "#     word_count = Counter(\" \".join(words.Word).split()).most_common(10)\n",
    "#     word_frequency = pd.DataFrame(word_count, columns = ['Word', 'Frequency'])\n",
    "#     print(word_frequency)\n",
    "#     print(\"Cluster \", i, \"'s most frequent word is: \", word_frequency.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn.externals.joblib\n",
    "import joblib\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
