{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/aspect-modelling-in-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT EXTRACTION\n",
      "\n",
      "[{'aspect': '', 'description': ''}, {'aspect': '', 'description': ''}, {'aspect': '', 'description': ''}, {'aspect': 'cake', 'description': ''}, {'aspect': 'cake', 'description': ''}, {'aspect': 'cake', 'description': ''}, {'aspect': 'cake', 'description': 'really tasty'}, {'aspect': '', 'description': ''}, {'aspect': 'party', 'description': ''}, {'aspect': 'party', 'description': ''}, {'aspect': 'party', 'description': 'amazing'}, {'aspect': 'party', 'description': 'amazing'}, {'aspect': '', 'description': ''}, {'aspect': 'mom', 'description': ''}, {'aspect': 'mom', 'description': ''}, {'aspect': 'mom', 'description': ''}, {'aspect': 'mom', 'description': 'best'}, {'aspect': 'mom', 'description': 'best'}, {'aspect': '', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': ''}, {'aspect': 'response', 'description': 'very enjoyable'}]\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import spacy\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Creating a list of positive and negative sentences.\n",
    "mixed_sen = [\n",
    "    'This chocolate truffle cake is really tasty',\n",
    "    'This party is amazing!',\n",
    "    'My mom is the best!',\n",
    "    'App response is very slow!'\n",
    "    'The trip to India was very enjoyable'\n",
    "]\n",
    "\n",
    "# An empty list for obtaining the extracted aspects\n",
    "# from sentences.\n",
    "ext_aspects = []\n",
    "\n",
    "# Performing Aspect Extraction\n",
    "for sen in mixed_sen:\n",
    "    important = sp(sen)\n",
    "    descriptive_item = ''\n",
    "    target = ''\n",
    "    for token in important:\n",
    "        if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "            target = token.text\n",
    "        if token.pos_ == 'ADJ':\n",
    "            added_terms = ''\n",
    "            for mini_token in token.children:\n",
    "                if mini_token.pos_ != 'ADV':\n",
    "                    continue\n",
    "                added_terms += mini_token.text + ' '\n",
    "            descriptive_item = added_terms + token.text\n",
    "        ext_aspects.append({'aspect': target,\n",
    "            'description': descriptive_item})\n",
    "\n",
    "print(\"ASPECT EXTRACTION\\n\")\n",
    "print(ext_aspects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Master of the\n",
      "[nltk_data]     House\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.076*\"sugar\" + 0.075*\"lifestyle\" + 0.075*\"expert\"'), (1, '0.065*\"driving\" + 0.065*\"father\" + 0.065*\"sister\"'), (2, '0.059*\"pressure\" + 0.059*\"sister\" + 0.059*\"father\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/tweets-classification-and-clustering-in-python-b107be1ba7c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import en_core_web_sm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# remove the hashtags, mentions and unwanted characters.\n",
    "def clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r”         (@[A-Za-z0–9]+)|([⁰-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?”, “”,   elem)) \n",
    "    return df\n",
    "tweets_bowl = clean_text(tweets_bowl, ‘tweets’)\n",
    "tweets_bowl.head()```\n",
    "\n",
    "nlp = en_core_web_sm.load() \n",
    "tokenizer = RegexpTokenizer(r’\\w+’)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words(‘english’))\n",
    "punctuation = list(string.punctuation) #already taken care of with the cleaning function.\n",
    "stop.update(punctuation)\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "def furnished(text):\n",
    "    final_text = []\n",
    "    for i in w_tokenizer.tokenize(text):\n",
    "        if i.lower() not in stop:\n",
    "            word = lemmatizer.lemmatize(i)\n",
    "            final_text.append(word.lower())\n",
    "    return “ “.join(final_text)\n",
    "tweets_bowl.tweets = tweets_bowl.tweets.apply(furnished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Identifying\n",
    "###### 7/6 - Assigment 4\n",
    "<p> We take the tweets out of the data, clean them of html, hashtags and mentions, then run the text through a tokenizer before identifying the most common words/phrases throughout the tweets. Those common words/phrases are then the aspects we base the clustering of the tweets on. As of right now, the model is using singular terms with a fixed dictionary, the hope is to move towards a dynamic dictionary that grows with the new data recieved.</p>\n",
    "\n",
    "###### 7/12 - Assigment 5\n",
    "<p>We are now showing the top 10 words per cluster and doing postprocessing on those words for cluster assignment. Instead of trying to assign one cluster at first, each tweet has been given a cluster ranking, indicating how much of a cluster each one is assigned to. Also, we have added in clustering by k-means to determine the amount of clusters there should be through a scientific and data-driven approach, rather than 5 for convenience.</p>\n",
    "\n",
    "7/19 - Assignment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p #https://pypi.org/project/tweet-preprocessor/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.programcreek.com/python/example/83254/sklearn.feature_extraction.text.TfidfVectorizer\n",
    "def create_union_model(params=None):\n",
    "    def preprocessor(tweet):\n",
    "        tweet = tweet.lower()\n",
    "\n",
    "        for k in emo_repl_order:\n",
    "            tweet = tweet.replace(k, emo_repl[k])\n",
    "        for r, repl in re_repl.iteritems():\n",
    "            tweet = re.sub(r, repl, tweet)\n",
    "\n",
    "        return tweet.replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "\n",
    "    tfidf_ngrams = TfidfVectorizer(preprocessor=preprocessor,\n",
    "                                   analyzer=\"word\")\n",
    "    ling_stats = LinguisticVectorizer()\n",
    "    all_features = FeatureUnion(\n",
    "        [('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
    "    #all_features = FeatureUnion([('tfidf', tfidf_ngrams)])\n",
    "    #all_features = FeatureUnion([('ling', ling_stats)])\n",
    "    clf = MultinomialNB()\n",
    "    pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
    "\n",
    "    if params:\n",
    "        pipeline.set_params(**params)\n",
    "\n",
    "    return pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Tweets.csv')\n",
    "tweets = df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets = []\n",
    "for tweet in tweets:\n",
    "    tweet = tweet.lower()\n",
    "    tweet = BeautifulSoup(tweet, 'html.parser').getText()\n",
    "    tweet = p.clean(tweet)\n",
    "    new_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = new_tweets\n",
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in tweets:\n",
    "    allwords_stemmed = tokenize_and_stem(i) # for each item in 'tweets', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) # extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jbencina/clustering-documents-with-tfidf-and-kmeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Code\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=10,\n",
    "                                   stop_words='english', max_features=10000,\n",
    "                                   strip_accents='unicode', use_idf=True, \n",
    "                                   tokenizer=tokenize_and_stem, ngram_range=(2,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(tweets) #fit the vectorizer to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'d like\", \"'d love\", \"'m fli\", \"'m flight\", \"'m just\", \"'m sorri\", \"'m stuck\", \"'m sure\", \"'m tri\", \"'s bag\", \"'s best\", \"'s cancel\", \"'s ceo\", \"'s ceo battl\", \"'s delay\", \"'s fleek\", \"'s flight\", \"'s good\", \"'s happen\", \"'s hour\", \"'s just\", \"'s new\", \"'s ok\", \"'s time\", \"'s whi\", 'admir club', 'agent help', \"agent n't\", 'agent phone', 'airlin cancel', 'airlin cancel flight', 'airlin flight', 'airlin ve', 'airport hour', 'alreadi cancel', 'american airlin', 'ani airlin', 'ani chanc', 'ani flight', 'ani help', 'ani idea', 'ani info', 'ani updat', 'ani way', 'anoth airlin', 'anoth flight', 'anoth hour', 'answer phone', 'anyon phone', 'app say', 'appeas passeng', 'appeas passeng wall', 'avail flight', 'awesom flight', 'awesom thank', 'bad weather', \"bag 's\", 'bag check', 'bag flight', 'bag lost', \"bag n't\", 'baggag claim', 'baggag fee', 'battl appeas', 'battl appeas passeng', 'becaus flight', \"becaus n't\", 'befor board', 'befor flight', 'best airlin', 'best flight', 'big thank', 'board flight', 'board pass', 'board plane', 'book flight', 'book onlin', 'book problem', 'book problem flight', 'book ticket', 'busi class', \"ca n't\", \"ca n't anyon\", \"ca n't believ\", \"ca n't book\", \"ca n't chang\", \"ca n't dm\", \"ca n't flight\", \"ca n't help\", \"ca n't phone\", \"ca n't use\", \"ca n't wait\", 'cancel flight', 'cancel flight flight', 'cancel flight reserv', 'cancel flightat', 'cancel flightl', 'cancel flightl flight', 'cancel flightl hold', 'cancel flightl ticket', 'cancel flightlat', 'car seat', 'care custom', 'caus miss', 'ceo battl', 'ceo battl appeas', 'chang fee', 'chang flight', 'chang onlin', 'chang reserv', 'chang ticket', 'check bag', 'check baggag', 'check flight', 'check onlin', 'class seat', 'companion pass', 'confirm number', 'connect flight', 'credit card', 'crew member', 'cross countri', 'cust servic', 'custom care', 'custom experi', 'custom relat', 'custom servic', 'custom servic agent', 'custom servic line', 'custom servic rep', 'custom servic today', 'day ago', 'day flight', 'day late', 'day vacat', 'delay becaus', 'delay cancel', 'delay cancel flightlat', 'delay flight', 'delay hour', 'delay hrs', 'delay miss', \"delay n't\", 'departur time', 'dfw cancel', 'dfw cancel flightl', \"did n't\", 'differ flight', 'direct flight', 'dividend mile', \"dm 'd\", 'dm confirm', 'dm sent', 'doe mean', \"doe n't\", \"doe n't help\", \"doe n't work\", 'earli bird', 'earlier flight', 'email address', 'email custom', 'empti seat', 'everi flight', 'everi singl', 'everi time', 'exit row', 'feel like', 'file claim', 'final board', 'final destin', 'final got', 'finger cross', \"fleet 's\", \"fleet 's fleek\", 'fli airlin', 'fli experi', 'fli guy', 'fli home', 'fli jetblu', 'fli southwest', 'fli time', 'fli unit', \"flight 'm\", \"flight 's\", 'flight aa', 'flight alreadi', 'flight arriv', 'flight attend', 'flight becaus', 'flight bna', 'flight board', 'flight book', 'flight book problem', 'flight bos', 'flight boston', 'flight ca', \"flight ca n't\", 'flight cancel', 'flight cancel flight', 'flight cancel flightl', 'flight chang', 'flight chicago', 'flight clt', 'flight come', 'flight crew', 'flight dalla', 'flight day', 'flight dca', 'flight delay', 'flight delay hour', 'flight depart', 'flight dfw', 'flight did', \"flight did n't\", 'flight ewr', 'flight experi', 'flight flight', 'flight fll', 'flight gate', 'flight got', 'flight got cancel', 'flight help', 'flight hold', 'flight home', 'flight hour', 'flight hrs', 'flight info', 'flight jfk', 'flight just', 'flight land', 'flight late', 'flight late flight', 'flight lax', 'flight leav', 'flight left', 'flight make', 'flight minut', 'flight miss', 'flight morn', \"flight n't\", 'flight nashvill', 'flight need', 'flight number', 'flight onli', 'flight onlin', 'flight ord', 'flight phl', 'flight phx', 'flight plan', 'flight plane', 'flight pleas', 'flight rebook', 'flight reschedul', 'flight reserv', 'flight san', 'flight say', 'flight schedul', 'flight sfo', 'flight status', 'flight thank', 'flight time', 'flight today', 'flight tomorrow', 'flight tonight', 'flight tri', 'flight use', 'flight ve', 'flight vega', 'flight veri', 'flight voucher', 'flight wait', 'flight want', 'flight way', 'flight weather', 'flight week', 'flight whi', 'flight yesterday', 'flightl flight', 'flightl hold', 'flightl ticket', 'follow dm', 'follow pleas', 'forc check', 'frequent flyer', 'gate agent', 'gate attend', 'gate chang', 'gate check', 'gate flight', 'gon na', 'good know', 'good luck', 'good work', 'got bag', 'got cancel', 'got cancel flightl', 'got disconnect', 'got email', 'got flight', 'got rebook', 'got ta', 'great custom', 'great custom servic', 'great flight', 'great job', 'great servic', 'great thank', 'ground crew', 'guy rock', 'half hour', 'help appreci', 'help custom', 'help flight', 'help need', 'help pleas', 'help rebook', 'hi just', 'high volum', 'hold custom', 'hold flight', 'hold hour', 'hold hrs', 'hold min', 'hold minut', 'hold time', 'home today', \"hope n't\", 'horribl experi', 'hotel room', 'hotel voucher', 'hour ago', 'hour befor', 'hour count', 'hour delay', 'hour flight', 'hour half', 'hour hold', 'hour late', 'hour late flight', 'hour late flightr', 'hour minut', 'hour phone', 'hour plane', 'hour tri', 'hour wait', 'hr delay', 'hr min', 'hr wait', 'hrs ago', 'hrs hold', 'hrs late', 'hrs late flightr', 'imagin dragon', 'intern flight', 'issu flight', 'jet blue', 'just cancel', 'just did', 'just got', 'just hang', 'just land', 'just like', 'just need', 'just say', 'just sent', 'just sent dm', 'just told', 'just tri', 'just wait', 'just want', \"know 's\", 'lack communic', 'las vega', 'late flight', 'late flightr', 'leav hour', \"let 's\", 'let know', 'let plane', 'line peopl', 'live person', 'll fli', 'll make', 'load bag', 'long day', 'long time', 'long wait', 'look forward', 'look like', 'lose bag', 'lose luggag', 'lost bag', 'lost baggag', 'lost custom', 'lost luggag', 'love fli', 'love guy', 'loyal custom', 'mainten issu', 'make chang', 'make connect', 'make flight', 'make right', 'make sens', 'make sure', 'mani time', 'mechan issu', 'mechan problem', 'middl seat', 'mile account', 'mileag plus', 'min befor', 'min count', 'min delay', 'min hold', 'min late', 'min late flight', 'min wait', 'minut ago', 'minut befor', 'minut count', 'minut hold', 'minut wait', 'miss connect', 'miss connect flight', 'miss flight', 'month ago', 'morn flight', 'multipl time', \"n't abl\", \"n't allow\", \"n't ani\", \"n't answer\", \"n't anyon\", \"n't anyth\", \"n't bag\", \"n't believ\", \"n't board\", \"n't book\", \"n't care\", \"n't chang\", \"n't charg\", \"n't check\", \"n't delay\", \"n't dm\", \"n't fli\", \"n't flight\", \"n't heard\", \"n't help\", \"n't hold\", \"n't just\", \"n't know\", \"n't leav\", \"n't left\", \"n't let\", \"n't like\", \"n't look\", \"n't make\", \"n't miss\", \"n't need\", \"n't phone\", \"n't realli\", \"n't rebook\", \"n't receiv\", \"n't say\", \"n't tell\", \"n't think\", \"n't understand\", \"n't use\", \"n't wait\", \"n't want\", \"n't work\", \"n't worri\", 'nd time', 'need assist', 'need better', 'need chang', 'need chang flight', 'need flight', 'need help', 'need home', 'need work', 'new flight', 'new plane', 'new york', 'ok thank', 'okay thank', 'onli airlin', 'onli flight', 'onli way', 'onlin flight', 'open seat', 'origin flight', 'overhead bin', 'passeng flight', 'passeng wall', 'passeng wall street', 'past day', 'past hour', 'pay custom', 'pay extra', 'pay hotel', 'peopl flight', 'peopl wait', 'peopl work', 'phone hold', 'phone hour', 'phone line', 'phone number', 'phone wait', 'pick phone', 'pilot flight', 'plane flight', 'plane gate', 'plane hour', 'plane just', 'plane sit', 'pleas dm', 'pleas fix', 'pleas follow', 'pleas help', 'pleas let', \"pleas n't\", 'pleas send', 'pleas tell', 'pleas thank', 'pls help', 'pm flight', 'poor custom', 'poor custom servic', 'problem flight', 'purchas ticket', 'quick respons', 'real person', 'realli need', 'rebook cancel', 'rebook flight', 'reflight book', 'reflight book problem', 'relat flightd', 'rent car', 'rental car', 'reschedul flight', 'return flight', 'round trip', 'rt fleet', \"rt fleet 's\", \"said n't\", 'san diego', 'san juan', 'sat plane', \"say 's\", 'say flight', 'seat assign', 'seat avail', 'seat flight', 'second time', 'send dm', 'send email', 'sent dm', 'sent email', 'sent thank', 'servic agent', 'servic desk', 'servic line', 'servic rep', 'servic suck', 'servic thank', 'servic today', 'servic ve', 'sever time', 'sit gate', 'sit plane', 'sit plane hour', 'sit runway', 'sit tarmac', 'sleep airport', 'social media', 'someon els', 'someon pleas', 'sound like', 'speak agent', 'speak human', 'speak someon', 'spent hour', 'st class', 'st flight', 'status flight', 'status match', 'stuck plane', 'stuck tarmac', 'suppos leav', 'sure whi', 'switch flight', 'taken care', 'talk someon', 'tell flight', 'terribl custom', 'terribl custom servic', 'terribl servic', \"thank 's\", 'thank cancel', 'thank did', 'thank follow', 'thank good', 'thank great', 'thank guy', 'thank help', 'thank info', 'thank just', 'thank ll', 'thank make', 'thank noth', 'thank quick', 'thank quick respons', 'thank repli', 'thank respond', 'thank respons', 'thank u', 'thank updat', 'thank veri', \"think 's\", 'think ll', 'ticket agent', 'ticket counter', 'time fli', 'time flight', 'time just', 'time min', 'time today', 'time ve', \"today 's\", 'today flight', 'told flight', 'tomorrow cancel', 'tomorrow cancel flightl', 'tomorrow morn', 'took care', 'travel advisori', 'travel experi', 'treat custom', 'tri book', 'tri book flight', 'tri chang', 'tri check', 'tri figur', 'tri flight', 'tri home', 'tri make', 'tri reach', 'tri rebook', 'u guy', 'u help', 'understand weather', 'understand whi', 'unit airlin', 'unit flight', 'updat flight', 'use credit', 've flown', 've got', 've hold', 've hold hour', 've seen', 've tri', 've wait', 'veri disappoint', 'veri frustrat', 'veri help', 'wait bag', 'wait flight', 'wait gate', 'wait hold', 'wait hour', 'wait hrs', 'wait line', 'wait min', 'wait minut', 'wait phone', 'wait plane', 'wait time', 'wall street', 'wan na', 'want chang', 'want fli', 'want home', 'want know', 'wast time', 'weather delay', 'weather issu', 'weather relat', 'weather relat flightd', 'websit say', 'week ago', 'whi ca', \"whi ca n't\", 'whi did', 'whi doe', 'whi flight', \"whi n't\", 'whi onli', \"wife 's\", \"wo n't\", \"wo n't let\", 'worst airlin', 'worst custom', 'worst custom servic', 'worst experi', 'worst fli', 'worst servic', 'year old', 'yes did', 'yes pleas', 'yes thank', 'yr old']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Code\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.47 s\n",
      "  (2, 146)\t1.0\n",
      "  (5, 146)\t1.0\n",
      "  (6, 625)\t0.7071067811865476\n",
      "  (6, 164)\t0.7071067811865476\n",
      "  (13, 2)\t1.0\n",
      "  (16, 232)\t0.7040194507073237\n",
      "  (16, 121)\t0.7101806904061515\n",
      "  (19, 684)\t1.0\n",
      "  (20, 554)\t1.0\n",
      "  (25, 588)\t1.0\n",
      "  (26, 702)\t0.5943321869893338\n",
      "  (26, 429)\t0.8042196537690904\n",
      "  (27, 472)\t1.0\n",
      "  (28, 431)\t0.8590490746027691\n",
      "  (28, 81)\t0.511893238306706\n",
      "  (29, 408)\t1.0\n",
      "  (30, 316)\t0.589724360740038\n",
      "  (30, 192)\t0.4456153511459782\n",
      "  (30, 191)\t0.4312458548660925\n",
      "  (30, 81)\t0.31625496656034874\n",
      "  (30, 77)\t0.40946543927360435\n",
      "  (36, 489)\t1.0\n",
      "  (37, 69)\t1.0\n",
      "  (38, 75)\t1.0\n",
      "  (41, 626)\t1.0\n",
      "  :\t:\n",
      "  (14627, 197)\t0.4076393846551074\n",
      "  (14627, 96)\t0.3499214287883302\n",
      "  (14627, 81)\t0.3489807168159074\n",
      "  (14627, 23)\t0.6276553008543857\n",
      "  (14628, 125)\t1.0\n",
      "  (14629, 107)\t1.0\n",
      "  (14632, 519)\t0.6098629620937192\n",
      "  (14632, 456)\t0.6578626857432153\n",
      "  (14632, 154)\t0.44190932799953253\n",
      "  (14633, 636)\t0.5103074665344262\n",
      "  (14633, 241)\t0.5813533971666575\n",
      "  (14633, 199)\t0.4028104856783904\n",
      "  (14633, 197)\t0.3712282044821285\n",
      "  (14633, 96)\t0.31866573400118897\n",
      "  (14635, 201)\t0.7071067811865475\n",
      "  (14635, 147)\t0.7071067811865475\n",
      "  (14636, 373)\t0.818054589286086\n",
      "  (14636, 183)\t0.5751405818997416\n",
      "  (14637, 35)\t1.0\n",
      "  (14638, 432)\t0.5692590431391985\n",
      "  (14638, 239)\t0.49778216568273215\n",
      "  (14638, 107)\t0.43848906831044904\n",
      "  (14638, 47)\t0.4856793122057881\n",
      "  (14639, 555)\t0.6983149970021153\n",
      "  (14639, 502)\t0.7157905873661204\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(terms)\n",
    "tfidf_vectorizer.fit(tweets)\n",
    "%time text = tfidf_vectorizer.transform(tweets)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit 2 clusters\n",
      "Fit 4 clusters\n",
      "Fit 6 clusters\n",
      "Fit 8 clusters\n",
      "Fit 10 clusters\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "find_optimal_clusters(text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_amt = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = MiniBatchKMeans(n_clusters=cluster_amt, init_size=1024, batch_size=2048, random_state=20).fit_predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = cluster_amt\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13563\n",
       "4      426\n",
       "5      322\n",
       "3      154\n",
       "2      111\n",
       "1       64\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets = {'tweet': tweets, 'cluster': clusters}\n",
    "\n",
    "#frame = pd.DataFrame(Tweets, clusters)\n",
    "frame = pd.DataFrame(Tweets)\n",
    "frame['Aspect'] = \"Miscellaneous\"\n",
    "frame['Customer Service'] = 0\n",
    "frame['Delays'] = 0\n",
    "frame['Cancelled Flight(s)'] = 0\n",
    "frame['Seating'] = 0\n",
    "frame['Baggage'] = 0\n",
    "\n",
    "\n",
    "#print(frame)\n",
    "\n",
    "frame['cluster'].value_counts() #number of tweets per cluster (clusters from 0 to 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_count = frame['cluster'].value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterwords = pd.DataFrame(columns = ['Cluster', 'Word', \"Reason\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspectsDF = pd.DataFrame(columns = ['Word', \"Reason\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0 words:"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"ca n't\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Code\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"ca n't\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-48881080a394>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0morder_centroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvocab_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mclusterwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusterwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Miscellaneous\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Code\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Code\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1964\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1965\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Code\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no slices here, handle elsewhere\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Code\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3535\u001b[0m             \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3536\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3537\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3539\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Code\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"ca n't\""
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(cluster_amt):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    name = \"Cluster %d words:\" % i\n",
    "    \n",
    "    for ind in order_centroids[i, :10]:\n",
    "        word = ' %s' % vocab_frame.loc[terms[ind]].values.tolist()[0][0]\n",
    "        print(word, end=',')\n",
    "        clusterwords.loc[len(clusterwords.index)] = [i, word, \"Miscellaneous\"]\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "aspect_words = []\n",
    "\n",
    "for i in range(cluster_amt):\n",
    "    for ind in order_centroids[i, :comparison_count]:\n",
    "        word = ' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0]\n",
    "        word = word.replace(\" \", \"\")\n",
    "        if word not in aspect_words:\n",
    "            aspect_words.append(word)\n",
    "            aspectsDF.loc[len(aspectsDF.index)] = [word, \"Miscellaneous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_assignments = {'Customer Service':['service', 'customer', 'phone', 'agent', 'email', 'speak', 'help', 'please', 'need', 'know', 'why', 'call'], \n",
    "                       'Delays':['delay', 'delayed', 'hour', 'hold', 'waited', 'late', 'status', 'schedule'], \n",
    "                       'Cancelled Flight(s)':['cancelled', 'rebook', 'missed', 'cancel', 'reschedule'], \n",
    "                       'Seating': ['book', 'booking', 'seats', 'seat', 'boarding'],\n",
    "                       'Baggage': ['bag', 'gate', 'check', 'lost', 'baggage', 'bags']\n",
    "                      }\n",
    "\n",
    "aspectsDF['Customer Service'] = 0\n",
    "aspectsDF['Delays'] = 0\n",
    "aspectsDF['Cancelled Flight(s)'] = 0\n",
    "aspectsDF['Seating'] = 0\n",
    "aspectsDF['Baggage'] = 0\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in aspect_assignments.items():\n",
    "        for item in value:\n",
    "            if (val == item):\n",
    "                return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(aspectsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(aspect_words)):\n",
    "    tof = False\n",
    "    word = aspect_words[i]\n",
    "    aspect_name = \"Miscellaneous\"\n",
    "    for value in aspect_assignments.values():\n",
    "        for item in value:\n",
    "            if (word == item):\n",
    "                tof = True\n",
    "                aspect_name = get_key(word)\n",
    "                break\n",
    "    aspectsDF.loc[i, 'Reason'] = aspect_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aspectsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    cs= 0\n",
    "    de= 0\n",
    "    cf= 0\n",
    "    se= 0\n",
    "    ba= 0\n",
    "    tweet_array = tweet.split(\" \")\n",
    "    for word in tweet_array:\n",
    "        if word in aspectsDF.values:\n",
    "            index = aspectsDF.index[aspectsDF['Word'] == word].tolist()[0]\n",
    "            aspect = aspectsDF.Reason.values[index]\n",
    "            if aspect == 'Customer Service':\n",
    "                cs += 1\n",
    "            if aspect == 'Delays':\n",
    "                de += 1\n",
    "            if aspect == 'Cancelled Flight(s)':\n",
    "                cf += 1\n",
    "            if aspect == 'Seating':\n",
    "                se += 1\n",
    "            if aspect == 'Baggage':\n",
    "                ba += 1\n",
    "                \n",
    "    assignments = {'cs': cs,'de': de,'cf': cf,'se': se,'ba': ba}\n",
    "    aspects = {'cs': 'Customer Service','de': 'Delays','cf': 'Cancelled Flight(s)','se': 'Seating','ba': 'Baggage', 'me': \"Miscellaneous\"}\n",
    "    test_value = max(assignments.values())\n",
    "    test_key = 'me'\n",
    "    if test_value == 0:\n",
    "        test_key = 'me'\n",
    "    for key, value in assignments.items():\n",
    "        if test_value == value:\n",
    "            test_key = key\n",
    "            break\n",
    "            \n",
    "    tweet_index = frame.index[frame['tweet'] == tweet].tolist()[0]\n",
    "    frame.loc[tweet_index, 'Aspect'] = aspects[test_key]\n",
    "    frame.loc[tweet_index, 'Customer Service'] = cs\n",
    "    frame.loc[tweet_index, 'Delays'] = de\n",
    "    frame.loc[tweet_index, 'Cancelled Flight(s)'] = cf\n",
    "    frame.loc[tweet_index, 'Seating'] = se\n",
    "    frame.loc[tweet_index, 'Baggage'] = ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Aspect'] = frame['Aspect']\n",
    "df['Customer Service'] = frame['Customer Service']\n",
    "df['Delays'] = frame['Delays']\n",
    "df['Cancelled Flight(s)'] = frame['Cancelled Flight(s)']\n",
    "df['Seating'] = frame['Seating']\n",
    "df['Baggage'] = frame['Baggage']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://codereview.stackexchange.com/questions/249329/finding-the-most-frequent-words-in-pandas-dataframe\n",
    "# for i in range(num_clusters):\n",
    "#     words = clusterwords[clusterwords.isin([i]).any(axis=1)]\n",
    "#     word_count = Counter(\" \".join(words.Word).split()).most_common(10)\n",
    "#     word_frequency = pd.DataFrame(word_count, columns = ['Word', 'Frequency'])\n",
    "#     print(word_frequency)\n",
    "#     print(\"Cluster \", i, \"'s most frequent word is: \", word_frequency.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clustered_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
